[["index.html", "Practical Statistics for Data Scientists Book Club Welcome", " Practical Statistics for Data Scientists Book Club The R4DS Online Learning Community 2021-06-02 Welcome This is a companion for the book Practical Statistics for Data Scientists by Peter Bruce, Andrew Bruce, and Peter Gedeck (O’Reilly, copyright 2020, 978-1-492-07294-2). This companion is available at r4ds.io/ps4ds. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["sample-code.html", "Sample code", " Sample code Sample code is available at github.com/gedeck/practical-statistics-for-data-scientists You can install all packages used by these notes and recommended by the book authors1: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r4ds/bookclub-ps4ds&quot;) remove.packages(&quot;ps4ds&quot;) # This isn&#39;t really a package. Chapter 5 uses {DMwR}, which is currently unavailable on CRAN. We’ll confirm when we get there, but I’m guessing {DMwR2} will work instead.↩︎ "],["st-edition-vs-2nd-edition.html", "1st edition vs 2nd edition", " 1st edition vs 2nd edition Here we’ll attempt to document differences between 1e and 2e. 2e added Python example code Some sections and subsections have slight name changes Chapter 1: New subsection in 1.6, “Probability” Chapter 2: New sections “Chi-Square Distribution” &amp; “F-Distribution” Chapter 4: New subsection in 4.2, “Further Reading” Chapter 7: New subsection in 7.1, “Correspondence Analysis” "],["pace.html", "Pace", " Pace Chapters are long, but not always dense. We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis", " Chapter 1 Exploratory Data Analysis Learning objectives: Classify data as numeric or categorical. Compare and contrast estimates of location. Compare and contrast estimates of variability. Visualize data distributions. Visualize categorical data. Use correlation coefficients to measure association between two variables. Visualize data distributions in two dimensions. "],["structured-data.html", "1.1 Structured Data", " 1.1 Structured Data Software classifies data by type. Numeric (continuous or discrete) Categorical (binary, ordinal, neither) Rectangular data = typical frame of reference for data science. Called a data.frame in R Rows are records (aka observations, cases, instances) Columns are features (aka variables, attributes, predictors in some cases) Lots of synonyms in stats and data science for same things. "],["estimates-of-location.html", "1.2 Estimates of Location", " 1.2 Estimates of Location Most basic = mean. dataset &lt;- c(3, 4, 1, 2, 10) mean(dataset) # (3 + 4 + 1 + 2 + 10)/5 = 20/5 ## [1] 4 Trimming helps eliminate outliers mean(dataset, trim = 1/5) # (2 + 3 + 4)/3 = 9/3 ## [1] 3 Weight to: Down-weight high-variability values. Up-weight under-represented values. weights &lt;- c(1, 1, 11, 1, 1) weighted.mean(dataset, weights) # (3 + 4 + 11 + 2 + 10)/15 = 30/15 ## [1] 2 Median: sort then choose middle value. median(dataset) # 1, 2, (3), 4, 10 ## [1] 3 Weighted median: similar to weighted mean, but more complicated. # Sort then weight then middle of weight. 1*11, 2*1, 3*1, 4*1, 10*1 matrixStats::weightedMedian(dataset, weights) ## [1] 1.333333 Technically it interpolates in-between values. matrixStats::weightedMedian(dataset, weights, interpolate = TRUE) ## [1] 1.333333 Can tell it not to interpolate to simplify. matrixStats::weightedMedian(dataset, weights, interpolate = FALSE) ## [1] 1 # Equivalent to repeating values weight times. median(c(rep(1, 11), 2, 3, 4, 10)) ## [1] 1 Their sample code is available at github.com/gedeck/practical-statistics-for-data-scientists "],["estimates-of-variability.html", "1.3 Estimates of Variability", " 1.3 Estimates of Variability Variability (aka dispersion) = are values clustered or spread out? 1.3.1 SD &amp; Friends Variance = average of squared deviations, \\(s^2 = \\frac{\\sum_{i=1}^{n}{(x_{1}-\\bar{x})^2}}{n-1}\\) s_squared &lt;- var(dataset) s_squared ## [1] 12.5 Standard deviation = square root of variance, \\(s = \\sqrt{variance}\\) s &lt;- sd(dataset) s ## [1] 3.535534 s == sqrt(s_squared) ## [1] TRUE Median absolute deviation from the median (MAD) is robust to outliers. mad(dataset) ## [1] 1.4826 Wait, why did that return the standard scale factor? dataset is c(1, 2, 3, 4, 10) The difference between any 2 values is 1 (except the outlier) 1 * 1.4826 = 1.4826 1.3.2 Percentiles &amp; Friends Percentiles = quantiles, \\(P\\%\\) of values are \\(&lt;= x\\) x &lt;- sample(1:100, 100, replace = TRUE) y &lt;- rnorm(100, mean = 50, sd = 20) quantile(x, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 4.0 10.8 23.8 33.0 43.6 49.5 61.0 71.3 79.4 92.1 100.0 quantile(y, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% ## 0.6076541 21.5563202 30.3286869 35.1392625 40.0179374 43.2925864 ## 60% 70% 80% 90% 100% ## 49.8647054 57.9024997 65.9658531 70.9103659 103.1880963 quantile(x) # quartile ## 0% 25% 50% 75% 100% ## 4.00 27.00 49.50 74.25 100.00 IQR(x) # They introduce this later but I like it here. ## [1] 47.25 "],["histograms-friends.html", "1.4 Histograms &amp; Friends", " 1.4 Histograms &amp; Friends state &lt;- read.csv(&quot;data/state.csv&quot;) head(state) ## State Population Murder.Rate Abbreviation ## 1 Alabama 4779736 5.7 AL ## 2 Alaska 710231 5.6 AK ## 3 Arizona 6392017 4.7 AZ ## 4 Arkansas 2915918 5.6 AR ## 5 California 37253956 4.4 CA ## 6 Colorado 5029196 2.8 CO library(ggplot2) ggplot(state, aes(y = Population/1000000)) + geom_boxplot() + ylab(&quot;Population (millions)&quot;) ggplot(state, aes(x = Population/1000000)) + geom_histogram( aes(y = after_stat(density)), bins = 10, fill = &quot;white&quot;, color = &quot;black&quot; ) + geom_density(fill = &quot;blue&quot;, alpha = 0.5) + xlab(&quot;Population (millions)&quot;) "],["visualizing-categorical-data.html", "1.5 Visualizing Categorical Data", " 1.5 Visualizing Categorical Data Bar charts are boring. We’ll see some examples related to this in 2D. "],["correlation.html", "1.6 Correlation", " 1.6 Correlation library(corrplot) ## corrplot 0.88 loaded library(dplyr, quietly = TRUE) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union sp500_px &lt;- read.csv(&quot;data/sp500_data.csv.gz&quot;) %&gt;% as_tibble() sp500_sym &lt;- read.csv(&quot;data/sp500_sectors.csv&quot;, stringsAsFactors = FALSE) %&gt;% as_tibble() etfs &lt;- sp500_px %&gt;% filter(X &gt; &quot;2012-07-01&quot;) %&gt;% select( any_of( sp500_sym %&gt;% filter(sector == &quot;etf&quot;) %&gt;% pull(symbol) ) ) corrplot(cor(etfs), method = &quot;ellipse&quot;) "],["d-distributions.html", "1.7 2D Distributions", " 1.7 2D Distributions https://xkcd.com/1967/ "],["meeting-videos.html", "1.8 Meeting Videos", " 1.8 Meeting Videos 1.8.1 Cohort 1 Meeting chat log CHAT LOG "],["data-and-sampling-distributions.html", "Chapter 2 Data and Sampling Distributions", " Chapter 2 Data and Sampling Distributions Learning objectives: Understand how to sample from a population. Identify various kinds of sampling bias. Know how to avoid bias in sampling. Understand the distribution of a sample statistic. Use the bootstrap to quantify sampling variability. Calculate confidence intervals. Recognize the most important common distributions. "],["what-is-a-population.html", "2.1 What is a Population?", " 2.1 What is a Population? roughly, “a particular set of things we care about” may be concrete the set of people who will vote in the next election all the trees in some forest more generally, is notional the space of outcomes from rolling a pair of dice all possible offspring from a mating pair of fruit flies the collection of physical microstates consistent with a given macrostate etc. "],["populations.html", "Populations", " Populations A population is represented as a distribution over one or more variables. voting populations are a distribution over the candidates tree populations are a distribution over the species of tree, the diameter of the trunk, the number of leaves, the thickness of the bark, etc. dice outcomes are a distribution over the number rolled etc. "],["population-statistics.html", "Population Statistics", " Population Statistics The things we care about are statistics that can be calculated from the distribution. the mode of the candidate distribution the median height of the trees, divided by the MAD of the number of branchings2 the mean and standard deviation of the number rolled etc. I am sure that nobody cares about this metric. The point is that a “statistic” can be any function of the distribution.↩︎ "],["what-is-a-sample.html", "2.2 What is a Sample?", " 2.2 What is a Sample? We almost never have full access to the population distribution that we care about, so we have to settle for a sample. consists of some number n of “individuals” from the population poll 2000 likely voters randomly select 50 trees from the forest to measure roll the dice 100 times drawn at random from the population represented by a distribution over the same variables as the population Whatever statistic we wanted to calculate for the population, we instead calculate for the sample. "],["what-is-a-sample-1.html", "What is a Sample?", " What is a Sample? The book gives the url for a helpful demo: https://onlinestatbook.com/stat_sim/sampling_dist/ "],["we-have-a-problem.html", "2.3 We Have a Problem", " 2.3 We Have a Problem The sample is not the population! The sample statistics we calculate are not equal to the population statistics! The sample statistic may differ from the population statistic for a variety of reasons: random fluctuation bias selection bias (the sample may not have been drawn randomly from the population) sample size bias3 (some sample metrics will be inherently and systematically different from population metrics just because of the limited size of the sample) the book glosses over this, so I don’t know if there’s a more standard term than just “bias”↩︎ "],["the-ideal-solution.html", "2.4 The Ideal Solution", " 2.4 The Ideal Solution If we had the resources to take many samples (e.g. 100 other research groups doing the same study of the forest that we are), then we could do the following: Repeat the sampling process some number of times, taking a new random sample each time. For each sample, calculate the sample statistic. Make a histogram of all the resulting values for the sample statistic. "],["the-ideal-solution-1.html", "The Ideal Solution", " The Ideal Solution The resulting sampling distribution of the statistic would help us understand the results of our sampling experiment. The mean of the sampling distribution is (an estimate of) the value of the statistic that our experiment is “aiming at.”4 The standard deviation of the distribution is (an estimate of) how much random fluctuations are likely to influence our measurement. This is also known as the standard error of our calculated sample statistic. This may not be equal to the population statistic, due to sample size bias. For example, if my target statistic is the range of some variable, the sample statistic will always be less than or equal to the population statistic. And so the mean of the sampling distribution for the range will be less than the population range. Some statistics, like the mean, are known to be unbiased statistics, while others are known to be biased.↩︎ "],["the-central-limit-theorem.html", "2.5 The Central Limit Theorem", " 2.5 The Central Limit Theorem In the special case where our sample statistic is the mean, it can be shown that: the sampling distribution approaches a normal distribution. an estimate of the standard deviation of that distribution is given by the standard deviation of an individual sample, divided by the square root of the sample size. "],["the-bootstrap-solution.html", "2.6 The Bootstrap Solution", " 2.6 The Bootstrap Solution If our sample statistic is the mean5, we can estimate the standard error from a single sample. Otherwise, to measure the standard error of our sample statistic, we would have to repeat our sampling process many times to be able to calculate the standard deviation of the sampling distribution. (Of course, if we actually did this, our calculated standard error would be pretty useless, because we’d have a better estimate of the statistic from the combined samples.) In practice, we can use a bootstrap. or other statistic for which a theoretical approximation to the standard error has been derived↩︎ "],["the-bootstrap.html", "The Bootstrap", " The Bootstrap The bootstrap is a simple but powerful technique for estimating the standard error of any sample statistic from a single sample. We will also obtain an estimate of the sample size bias for our sample statistic. Sounds too good to be true. Where does this amazing capability come from? "],["the-bootstrap-1.html", "The Bootstrap", " The Bootstrap The bootstrap works like this: Assume that the sample distribution is representative of the population distribution.6 Construct a new simulated population by making a gazillion copies of your sample (in practice, this just means you draw from the sample with replacement). Now it is easy to do lots of simulated experiments on your simulated population! Draw lots of bootstrap samples from your simulated population, with each sample having the same n as your original sample. Calculate your sample statistic for each of the bootstrap samples. Calculate the standard deviation of the resulting distribution (of bootstrapped sample statistics); this is the estimated standard error for your measured sample statistic. Calculate the difference between the mean of the bootstrap distribution and the statistic calculated on your original sample; this is the estimated bias for your measurement. We can use the same sampling demo to get a feel for how bootstrapping works. It doesn’t have to be perfect, but if it’s too far off then nothing you calculate from the sample will be meaningful anyway.↩︎ "],["the-bootstrap-limitations.html", "The Bootstrap: Limitations", " The Bootstrap: Limitations The bootstrap isn’t magic. It won’t give you a better estimate of your sample statistic.7 It won’t fix selection bias. It won’t fill gaps in your sample data. It does help you understand the limitations of your experimental procedure. Well, having an estimate of the sample size bias can help.↩︎ "],["confidence-intervals.html", "2.7 Confidence Intervals", " 2.7 Confidence Intervals A confidence interval, like the standard error, is a way to estimate the reliability of a sample statistic. For example, a 95% CI is an interval that would contain the central 95% of values for the sample statistic, if the sampling experiment were done a very large number of times. It’s generally not practical to actually sample that many times, so… bootstrap! For comparison, plus or minus one SE gives a CI of about 68%. "],["some-important-distributions.html", "2.8 Some Important Distributions", " 2.8 Some Important Distributions The rest of the chapter is about specific distributions. Many of these distributions will come up again in particular contexts in following chapters. "],["the-normal-distribution.html", "2.9 The Normal Distribution", " 2.9 The Normal Distribution normal_values &lt;- rnorm(n = 10000) hist(normal_values, breaks = 30) # qqnorm plots the location of quantiles of the given distribution # vs locations of corresponding quantiles of normal distribution. qqnorm(normal_values); qqline(normal_values, col = 2) "],["students-t-distribution.html", "2.10 Student’s t-Distribution", " 2.10 Student’s t-Distribution # t-distribution is a family parameterized by degrees of freedom t_values &lt;- rt(n = 10000, df = 10) hist(t_values, breaks = 30) # normalize distribution for QQ t_values &lt;- (t_values - mean(t_values))/sd(t_values) qqnorm(t_values); qqline(t_values, col = 2) "],["the-binomial-distribution.html", "2.11 The Binomial Distribution", " 2.11 The Binomial Distribution # flipping 6 fair coins at a time, how many heads do we get? binom_values &lt;- rbinom(n = 10000, 6, 0.5) hist(binom_values, breaks = 30) # normalize distribution for QQ binom_values &lt;- (binom_values - mean(binom_values))/sd(binom_values) qqnorm(binom_values); qqline(binom_values, col = 2) "],["the-chi-square-distribution.html", "2.12 The Chi-Square Distribution", " 2.12 The Chi-Square Distribution # family of distributions parameterized by degrees of freedom chisq_values &lt;- rchisq(n = 10000, df = 5) hist(chisq_values, breaks = 30) # normalize distribution for QQ chisq_values &lt;- (chisq_values - mean(chisq_values))/sd(chisq_values) qqnorm(chisq_values); qqline(chisq_values, col = 2) "],["the-f-distribution.html", "2.13 The F-Distribution", " 2.13 The F-Distribution # family of distributions parameterized by TWO df1 values f_values &lt;- rf(n = 10000, df1 = 15, df2 = 50) hist(f_values, breaks = 30) # normalize distribution for QQ f_values &lt;- (f_values - mean(f_values))/sd(f_values) qqnorm(f_values); qqline(f_values, col = 2) "],["the-poisson-distribution.html", "2.14 The Poisson Distribution", " 2.14 The Poisson Distribution # family of distributions parameterized by lambda (&quot;mean rate&quot;) poisson_values &lt;- rpois(n = 10000, lambda = 5) hist(poisson_values, breaks = 30) # normalize distribution for QQ poisson_values &lt;- (poisson_values - mean(poisson_values))/sd(poisson_values) qqnorm(poisson_values); qqline(poisson_values, col = 2) "],["meeting-videos-1.html", "2.15 Meeting Videos", " 2.15 Meeting Videos 2.15.1 Cohort 1 Meeting chat log CHAT LOG "],["statistical-experiments-and-significance-testing.html", "Chapter 3 Statistical Experiments and Significance Testing", " Chapter 3 Statistical Experiments and Significance Testing Main point: resampling is the best way to test the significance of a statistical experiment. Learning objectives: Design A/B tests. Use hypothesis tests to understand the results of a statistical experiment. Perform resampling procedures to test the significance of statistical experiments. Explain the proper uses (and abuses!) of p-values. Compare and contrast the various traditional tests of significance. Explain the advantages of multi-armed bandit tests over traditional A/B tests. Calculate the statistical power for a statistical experiment. "],["slide-2.html", "3.1 SLIDE 2", " 3.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3.html", "3.2 SLIDE 3", " 3.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-2.html", "3.3 Meeting Videos", " 3.3 Meeting Videos 3.3.1 Cohort 1 Meeting chat log CHAT LOG "],["regression-and-prediction.html", "Chapter 4 Regression and Prediction", " Chapter 4 Regression and Prediction Learning objectives: TBD "],["slide-2-1.html", "4.1 SLIDE 2", " 4.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-1.html", "4.2 SLIDE 3", " 4.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-3.html", "4.3 Meeting Videos", " 4.3 Meeting Videos 4.3.1 Cohort 1 Meeting chat log CHAT LOG "],["classification.html", "Chapter 5 Classification", " Chapter 5 Classification Learning objectives: TBD "],["slide-2-2.html", "5.1 SLIDE 2", " 5.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-2.html", "5.2 SLIDE 3", " 5.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-4.html", "5.3 Meeting Videos", " 5.3 Meeting Videos 5.3.1 Cohort 1 Meeting chat log CHAT LOG "],["statistical-machine-learning.html", "Chapter 6 Statistical Machine Learning", " Chapter 6 Statistical Machine Learning Learning objectives: TBD "],["slide-2-3.html", "6.1 SLIDE 2", " 6.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-3.html", "6.2 SLIDE 3", " 6.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-5.html", "6.3 Meeting Videos", " 6.3 Meeting Videos 6.3.1 Cohort 1 Meeting chat log CHAT LOG "],["unsupervised-learning.html", "Chapter 7 Unsupervised Learning", " Chapter 7 Unsupervised Learning Learning objectives: TBD "],["slide-2-4.html", "7.1 SLIDE 2", " 7.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-4.html", "7.2 SLIDE 3", " 7.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-6.html", "7.3 Meeting Videos", " 7.3 Meeting Videos 7.3.1 Cohort 1 Meeting chat log CHAT LOG "]]
